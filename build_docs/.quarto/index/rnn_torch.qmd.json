{"title":"RNN in Torch","markdown":{"yaml":"---\ntitle: \"RNN in Torch\"\n---","headingText":"Overview","containsRefs":false,"markdown":"\n\n\nThis tutorial demonstrates how the Recurrent Neural Network underlying Deep VAR is set up using [torch for R](https://torch.mlverse.org/). It heavily draws on ideas and code presented in this great [tutorial](https://blogs.rstudio.com/ai/posts/2021-03-11-forecasting-time-series-with-torch_2/) from the RStudio AI blog. \n\n## Data input\n\n```{r}\ntrain_val_test_split <- function(data, train_size=0.8) {\n  N <- nrow(data)\n  end_train <- round(train_size * N)\n  end_val <- end_train + round((N - end_train)/2)\n  data_train <- data[1:end_train,] |> as.matrix()\n  data_val <- data[(end_train+1):end_val,] |> as.matrix()\n  data_test <- data[(end_val+1):N,] |> as.matrix()\n  return(list(train=data_train, val=data_val, test=data_test))\n}\n```\n\n\n```{r}\nlibrary(deepvars)\nlibrary(data.table)\ndata(\"canada\")\ndt <- data.table(canada)\nvar_cols = colnames(dt)[2:ncol(dt)]\ndt[,(var_cols) := lapply(.SD, function(i) c(0,diff(i))), .SDcols=var_cols]\nsplits_ <- train_val_test_split(dt[,-1], train_size = 0.5)\ndf_train <- splits_$train\ndf_val <- splits_$val\ndf_test <- splits_$test\n```\n\nRecall that the neural network that we are aiming to build takes as its input its own $p$ lags as well as $p$ lags of all other variables in the system.\n\n```{r, eval=TRUE}\nlibrary(torch)\n\ndvar_dataset <- dataset(\n  name = \"dvar_dataset\",\n  \n  initialize = function(X, response, lags, n_ahead, sample_frac = 1) {\n    \n    self$lags <- lags\n    self$n_ahead <- n_ahead\n    self$response <- response\n    self$train_mean <- colMeans(X)\n    self$train_sd <- sapply(1:ncol(X), function(i) sd(X[,i]))\n    self$X <- torch_tensor(t((t(X) - self$train_mean)/self$train_sd)) # of dimension (D x T)\n    \n    n <- dim(self$X)[1] - self$lags - self$n_ahead + 1\n    \n    self$starts <- sort(sample.int(\n      n = n,\n      size = round(n * sample_frac)\n    ))\n    \n  },\n  \n  .getitem = function(i) {\n    \n    start <- self$starts[i]\n    end <- start + self$lags - 1\n    pred_length <- self$n_ahead\n    \n    list(\n      X = self$X[start:end,],\n      y = self$X[(end + 1):(end + pred_length),self$response]\n    )\n    \n  },\n  \n  .length = function() {\n    length(self$starts) \n  }\n)\n```\n\n```{r}\nset.seed(123)\nresponse_var_idx <- 1\nlags <- 6\nn_ahead <- 12\ntrain_ds <- dvar_dataset(df_train, response_var_idx, lags, n_ahead = n_ahead, sample_frac = 0.5)\n\nbatch_size <- 30\ntrain_dl <- train_ds %>% dataloader(batch_size = batch_size, shuffle = TRUE)\n\nvalid_ds <- dvar_dataset(df_val, response_var_idx, lags, n_ahead=n_ahead, sample_frac = 0.5)\nvalid_dl <- valid_ds %>% dataloader(batch_size = batch_size)\n\ntest_ds <- dvar_dataset(df_test, response_var_idx, lags, n_ahead=n_ahead)\ntest_dl <- test_ds %>% dataloader(batch_size = 1)\n```\n\nLet's do a quick sanity check to see if the dimensions check out:\n\n```{r}\ntrain_ds[1]\n```\n\nThis looks like what we expected: `X` is a $(D \\times p)$ tensor and $y$ is just the single output.\n\n```{r}\nin_idx <- train_ds$starts[1]:(train_ds$starts[1]+train_ds$lags-1)\nout_idx <- c(max(in_idx)+1,response_var_idx)\nX_train <- t((t(df_train) - colMeans(df_train)) / sapply(1:ncol(df_train), function(i) sd(df_train[,i])))\nX_train[in_idx,]\nX_train[out_idx[1]:(out_idx[1]+n_ahead-1),out_idx[2]]\n```\n\nWhat about the mini-batch? We see that `X` is of the desired dimension `(batch_size, n_timesteps, num_features)`.\n\n```{r}\nlength(train_dl)\n\nb <- train_dl %>% dataloader_make_iter() %>% dataloader_next()\nb\n```\n\n## Model\n\n```{r}\nmodel <- nn_module(\n  \n  initialize = function(type=\"lstm\", input_size, hidden_size, linear_size, output_size,\n                        num_layers = 2, dropout = 0.25, linear_dropout = 0.25) {\n    \n    self$type <- type\n    self$num_layers <- num_layers\n    self$linear_dropout <- linear_dropout\n    \n    self$rnn <- if (self$type == \"gru\") {\n      nn_gru(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    } else {\n      nn_lstm(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    }\n    \n    self$mlp <- nn_sequential(\n      nn_linear(hidden_size, linear_size),\n      nn_relu(),\n      nn_dropout(linear_dropout),\n      nn_linear(linear_size, output_size)\n    )\n    \n  },\n  \n  forward = function(x) {\n    \n    x <- self$rnn(x)\n    x[[1]][ ,-1, ..] %>% \n      self$mlp()\n    \n  }\n  \n)\n```\n\n```{r}\nD <- dim(train_ds[1]$X)[2]\nnet <- model(input_size = D, hidden_size = 32, linear_size = 128, output_size = n_ahead)\ndevice <- torch_device(if (cuda_is_available()) \"cuda\" else \"cpu\")\nnet <- net$to(device = device)\n```\n\n## Training\n\n```{r}\noptimizer <- optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs <- 30\n\ntrain_batch <- function(b) {\n  \n  optimizer$zero_grad() # in\n  output <- net(b$X$to(device = device))\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$backward()\n  optimizer$step()\n  \n  loss$item()\n}\n\nvalid_batch <- function(b) {\n  \n  output <- net(b$X$to(device = device))\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$item()\n  \n}\n\nfor (epoch in 1:num_epochs) {\n  \n  net$train()\n  train_loss <- c()\n  \n  coro::loop(for (b in train_dl) {\n    loss <- train_batch(b)\n    train_loss <- c(train_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n  \n  net$eval()\n  valid_loss <- c()\n  \n  coro::loop(for (b in valid_dl) {\n    loss <- valid_batch(b)\n    valid_loss <- c(valid_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n```\n\n\n## Evaluation\n\nBelow we see the `r n_ahead`-step prediction\n\n```{r, eval=TRUE, warning=FALSE}\nnet$eval()\n\ntest_preds <- vector(mode = \"list\", length = length(test_dl))\n\ni <- 1\n\ncoro::loop(for (b in test_dl) {\n  \n  input <- b$X\n  output <- net(input$to(device = device))\n  preds <- as.numeric(output)\n  \n  test_preds[[i]] <- preds\n  i <<- i + 1\n  \n})\n\ny_hat <- test_preds[[1]] * train_ds$train_sd[response_var_idx] + train_ds$train_mean[response_var_idx]\ny_hat <- data.table(y_hat)[,id:=1:.N][,variable:=colnames(df_train)[response_var_idx]]\nsetkey(y_hat, id, variable)\n\nlibrary(data.table)\ny_true <- data.table(rbind(df_val,df_test))\ny_true <- y_true[,..response_var_idx]\ny_true[,id:=(-nrow(df_val)-lags+1):(-nrow(df_val)-lags+y_true[,.N])]\ny_true <- melt(y_true, id.vars = \"id\", value.name = \"y\")\nsetkey(y_true, id, variable)\n\ndt_plot <- melt(y_hat[y_true],id.vars = c(\"id\",\"variable\"), variable.name = \"type\")\ndt_plot[type==\"y_hat\" & id==0, value:=dt_plot[type==\"y\" & id==0]$value]\nlibrary(ggplot2)\n\nggplot(dt_plot[id<=n_ahead], aes(x=id, y=value, colour=type)) +\n  geom_line() +\n  geom_point() +\n  scale_color_manual(name=\"Type:\", values=c(\"blue\", \"red\"), labels=c(\"Prediction\", \"Actual\")) +\n  labs(\n    title = sprintf(\"Variable: %s\", colnames(df_train)[response_var_idx]),\n    x = \"Time\",\n    y = \"Value\"\n  )\n  \n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"merge-includes":true,"self-contained-math":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false},"pandoc":{"standalone":true,"to":"html","css":["styles.css"],"toc":true,"output-file":"rnn_torch.html"},"language":{},"metadata":{"fig-responsive":true,"theme":"cosmo","title":"RNN in Torch"},"extensions":{"book":{"multiFile":true}}}}}